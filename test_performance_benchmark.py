"""
æ€§èƒ½åŸºå‡†æµ‹è¯•
æµ‹è¯•æœŸæœ›å¯¹æ¯”åŠŸèƒ½å¯¹ç³»ç»Ÿæ€§èƒ½çš„å½±å“

Requirements: 8.5
- æµ‹è¯•ä¸åŒæ•°æ®é‡ä¸‹çš„æ€§èƒ½è¡¨çŽ°
- å¯¹æ¯”æ–°åŠŸèƒ½æ·»åŠ å‰åŽçš„æ€§èƒ½å·®å¼‚
- éªŒè¯ç³»ç»Ÿåœ¨é«˜è´Ÿè½½ä¸‹çš„ç¨³å®šæ€§
"""

import time
import statistics
import json
import threading
import psutil
import os
from datetime import datetime, timedelta
from flask import Flask
from contextlib import contextmanager
from models.trade_record import TradeRecord
from services.analytics_service import AnalyticsService
from services.expectation_comparison_service import ExpectationComparisonService


class PerformanceBenchmark:
    """æ€§èƒ½åŸºå‡†æµ‹è¯•ç±»"""
    
    def __init__(self):
        self.app = Flask(__name__)
        self.results = {}
        self.process = psutil.Process(os.getpid())
        
    @contextmanager
    def measure_performance(self, test_name):
        """æ€§èƒ½æµ‹é‡ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        # è®°å½•å¼€å§‹çŠ¶æ€
        start_time = time.time()
        start_memory = self.process.memory_info().rss
        start_cpu = self.process.cpu_percent()
        
        try:
            yield
        finally:
            # è®°å½•ç»“æŸçŠ¶æ€
            end_time = time.time()
            end_memory = self.process.memory_info().rss
            end_cpu = self.process.cpu_percent()
            
            # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
            duration = end_time - start_time
            memory_delta = end_memory - start_memory
            cpu_usage = (start_cpu + end_cpu) / 2
            
            self.results[test_name] = {
                'duration': duration,
                'memory_delta': memory_delta,
                'cpu_usage': cpu_usage,
                'timestamp': datetime.now().isoformat()
            }
    
    def run_benchmark_suite(self):
        """è¿è¡Œå®Œæ•´çš„æ€§èƒ½åŸºå‡†æµ‹è¯•å¥—ä»¶"""
        
        print("å¼€å§‹æ€§èƒ½åŸºå‡†æµ‹è¯•...")
        print("=" * 60)
        
        with self.app.app_context():
            # 1. åŸºç¡€APIæ€§èƒ½æµ‹è¯•
            self.test_basic_api_performance()
            
            # 2. æœŸæœ›å¯¹æ¯”åŠŸèƒ½æ€§èƒ½æµ‹è¯•
            self.test_expectation_comparison_performance()
            
            # 3. å¹¶å‘è®¿é—®æ€§èƒ½æµ‹è¯•
            self.test_concurrent_access_performance()
            
            # 4. å¤§æ•°æ®é‡æ€§èƒ½æµ‹è¯•
            self.test_large_dataset_performance()
            
            # 5. å†…å­˜ä½¿ç”¨æµ‹è¯•
            self.test_memory_usage()
            
            # 6. ç¼“å­˜æ€§èƒ½æµ‹è¯•
            self.test_caching_performance()
        
        # ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š
        self.generate_performance_report()
    
    def test_basic_api_performance(self):
        """æµ‹è¯•åŸºç¡€APIæ€§èƒ½"""
        
        print("æµ‹è¯•åŸºç¡€APIæ€§èƒ½...")
        
        # æµ‹è¯•æ€»ä½“ç»Ÿè®¡API
        with self.measure_performance('analytics_overview'):
            for _ in range(10):
                AnalyticsService.get_overall_statistics()
        
        # æµ‹è¯•æ”¶ç›Šåˆ†å¸ƒAPI
        with self.measure_performance('profit_distribution'):
            for _ in range(10):
                AnalyticsService.get_profit_distribution()
        
        # æµ‹è¯•æœˆåº¦ç»Ÿè®¡API
        with self.measure_performance('monthly_statistics'):
            for _ in range(10):
                AnalyticsService.get_monthly_statistics(datetime.now().year)
        
        print("âœ“ åŸºç¡€APIæ€§èƒ½æµ‹è¯•å®Œæˆ")
    
    def test_expectation_comparison_performance(self):
        """æµ‹è¯•æœŸæœ›å¯¹æ¯”åŠŸèƒ½æ€§èƒ½"""
        
        print("æµ‹è¯•æœŸæœ›å¯¹æ¯”åŠŸèƒ½æ€§èƒ½...")
        
        time_ranges = ['30d', '90d', '1y', 'all']
        
        for time_range in time_ranges:
            test_name = f'expectation_comparison_{time_range}'
            
            with self.measure_performance(test_name):
                for _ in range(5):
                    ExpectationComparisonService.get_expectation_comparison(time_range)
        
        print("âœ“ æœŸæœ›å¯¹æ¯”åŠŸèƒ½æ€§èƒ½æµ‹è¯•å®Œæˆ")
    
    def test_concurrent_access_performance(self):
        """æµ‹è¯•å¹¶å‘è®¿é—®æ€§èƒ½"""
        
        print("æµ‹è¯•å¹¶å‘è®¿é—®æ€§èƒ½...")
        
        def concurrent_request():
            """å¹¶å‘è¯·æ±‚å‡½æ•°"""
            try:
                # éšæœºè°ƒç”¨ä¸åŒçš„API
                import random
                apis = [
                    lambda: AnalyticsService.get_overall_statistics(),
                    lambda: AnalyticsService.get_profit_distribution(),
                    lambda: ExpectationComparisonService.get_expectation_comparison('all'),
                    lambda: AnalyticsService.get_monthly_statistics(datetime.now().year)
                ]
                
                api = random.choice(apis)
                api()
                
            except Exception as e:
                print(f"å¹¶å‘è¯·æ±‚å¼‚å¸¸: {e}")
        
        # æµ‹è¯•ä¸åŒå¹¶å‘çº§åˆ«
        concurrency_levels = [5, 10, 20]
        
        for level in concurrency_levels:
            test_name = f'concurrent_access_{level}_threads'
            
            with self.measure_performance(test_name):
                threads = []
                
                for _ in range(level):
                    thread = threading.Thread(target=concurrent_request)
                    threads.append(thread)
                    thread.start()
                
                for thread in threads:
                    thread.join()
        
        print("âœ“ å¹¶å‘è®¿é—®æ€§èƒ½æµ‹è¯•å®Œæˆ")
    
    def test_large_dataset_performance(self):
        """æµ‹è¯•å¤§æ•°æ®é‡æ€§èƒ½"""
        
        print("æµ‹è¯•å¤§æ•°æ®é‡æ€§èƒ½...")
        
        # èŽ·å–å½“å‰æ•°æ®é‡
        trade_count = TradeRecord.query.count()
        
        print(f"å½“å‰äº¤æ˜“è®°å½•æ•°é‡: {trade_count}")
        
        # æµ‹è¯•ä¸åŒæ•°æ®é‡ä¸‹çš„æ€§èƒ½
        with self.measure_performance('large_dataset_all_time'):
            # å…¨æ—¶é—´èŒƒå›´æŸ¥è¯¢
            ExpectationComparisonService.get_expectation_comparison('all')
        
        with self.measure_performance('large_dataset_complex_query'):
            # å¤æ‚æŸ¥è¯¢ç»„åˆ
            AnalyticsService.get_overall_statistics()
            AnalyticsService.get_profit_distribution()
            ExpectationComparisonService.get_expectation_comparison('1y')
        
        print("âœ“ å¤§æ•°æ®é‡æ€§èƒ½æµ‹è¯•å®Œæˆ")
    
    def test_memory_usage(self):
        """æµ‹è¯•å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        
        print("æµ‹è¯•å†…å­˜ä½¿ç”¨æƒ…å†µ...")
        
        initial_memory = self.process.memory_info().rss
        
        # è¿žç»­æ‰§è¡Œå¤šæ¬¡APIè°ƒç”¨
        with self.measure_performance('memory_stress_test'):
            for i in range(50):
                AnalyticsService.get_overall_statistics()
                ExpectationComparisonService.get_expectation_comparison('all')
                
                # æ¯10æ¬¡æ£€æŸ¥ä¸€æ¬¡å†…å­˜
                if i % 10 == 0:
                    current_memory = self.process.memory_info().rss
                    memory_increase = current_memory - initial_memory
                    
                    print(f"  ç¬¬{i+1}æ¬¡è°ƒç”¨åŽå†…å­˜å¢žé•¿: {memory_increase / 1024 / 1024:.2f}MB")
                    
                    # å¦‚æžœå†…å­˜å¢žé•¿è¶…è¿‡100MBï¼Œå¯èƒ½å­˜åœ¨å†…å­˜æ³„æ¼
                    if memory_increase > 100 * 1024 * 1024:
                        print(f"âš ï¸  è­¦å‘Š: å†…å­˜å¢žé•¿è¿‡å¤š ({memory_increase / 1024 / 1024:.2f}MB)")
                        break
        
        final_memory = self.process.memory_info().rss
        total_increase = final_memory - initial_memory
        
        self.results['memory_usage_summary'] = {
            'initial_memory_mb': initial_memory / 1024 / 1024,
            'final_memory_mb': final_memory / 1024 / 1024,
            'total_increase_mb': total_increase / 1024 / 1024
        }
        
        print("âœ“ å†…å­˜ä½¿ç”¨æµ‹è¯•å®Œæˆ")
    
    def test_caching_performance(self):
        """æµ‹è¯•ç¼“å­˜æ€§èƒ½"""
        
        print("æµ‹è¯•ç¼“å­˜æ€§èƒ½...")
        
        # ç¬¬ä¸€æ¬¡è°ƒç”¨ï¼ˆå†·ç¼“å­˜ï¼‰
        with self.measure_performance('cold_cache_call'):
            ExpectationComparisonService.get_expectation_comparison('all')
        
        # ç¬¬äºŒæ¬¡è°ƒç”¨ï¼ˆå¯èƒ½çš„çƒ­ç¼“å­˜ï¼‰
        with self.measure_performance('warm_cache_call'):
            ExpectationComparisonService.get_expectation_comparison('all')
        
        # è¿žç»­è°ƒç”¨æµ‹è¯•
        with self.measure_performance('repeated_calls'):
            for _ in range(10):
                ExpectationComparisonService.get_expectation_comparison('all')
        
        print("âœ“ ç¼“å­˜æ€§èƒ½æµ‹è¯•å®Œæˆ")
    
    def generate_performance_report(self):
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        
        print("\n" + "=" * 60)
        print("æ€§èƒ½æµ‹è¯•æŠ¥å‘Š")
        print("=" * 60)
        
        # æŒ‰ç±»åˆ«ç»„ç»‡ç»“æžœ
        categories = {
            'åŸºç¡€APIæ€§èƒ½': ['analytics_overview', 'profit_distribution', 'monthly_statistics'],
            'æœŸæœ›å¯¹æ¯”åŠŸèƒ½': ['expectation_comparison_30d', 'expectation_comparison_90d', 
                         'expectation_comparison_1y', 'expectation_comparison_all'],
            'å¹¶å‘è®¿é—®æ€§èƒ½': ['concurrent_access_5_threads', 'concurrent_access_10_threads', 
                         'concurrent_access_20_threads'],
            'å¤§æ•°æ®é‡æ€§èƒ½': ['large_dataset_all_time', 'large_dataset_complex_query'],
            'å†…å­˜å’Œç¼“å­˜': ['memory_stress_test', 'cold_cache_call', 'warm_cache_call', 'repeated_calls']
        }
        
        for category, tests in categories.items():
            print(f"\n{category}:")
            print("-" * 40)
            
            for test in tests:
                if test in self.results:
                    result = self.results[test]
                    duration = result['duration']
                    memory_mb = result['memory_delta'] / 1024 / 1024
                    cpu = result['cpu_usage']
                    
                    print(f"  {test}:")
                    print(f"    è€—æ—¶: {duration:.3f}ç§’")
                    print(f"    å†…å­˜å˜åŒ–: {memory_mb:+.2f}MB")
                    print(f"    CPUä½¿ç”¨: {cpu:.1f}%")
        
        # æ€§èƒ½æ‘˜è¦
        print(f"\næ€§èƒ½æ‘˜è¦:")
        print("-" * 40)
        
        all_durations = [r['duration'] for r in self.results.values() if 'duration' in r]
        if all_durations:
            print(f"  å¹³å‡å“åº”æ—¶é—´: {statistics.mean(all_durations):.3f}ç§’")
            print(f"  æœ€å¿«å“åº”æ—¶é—´: {min(all_durations):.3f}ç§’")
            print(f"  æœ€æ…¢å“åº”æ—¶é—´: {max(all_durations):.3f}ç§’")
            print(f"  å“åº”æ—¶é—´æ ‡å‡†å·®: {statistics.stdev(all_durations):.3f}ç§’")
        
        # å†…å­˜ä½¿ç”¨æ‘˜è¦
        if 'memory_usage_summary' in self.results:
            mem_summary = self.results['memory_usage_summary']
            print(f"  åˆå§‹å†…å­˜: {mem_summary['initial_memory_mb']:.2f}MB")
            print(f"  æœ€ç»ˆå†…å­˜: {mem_summary['final_memory_mb']:.2f}MB")
            print(f"  å†…å­˜å¢žé•¿: {mem_summary['total_increase_mb']:.2f}MB")
        
        # æ€§èƒ½è¯„çº§
        self.calculate_performance_rating()
        
        # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        self.save_detailed_report()
    
    def calculate_performance_rating(self):
        """è®¡ç®—æ€§èƒ½è¯„çº§"""
        
        print(f"\næ€§èƒ½è¯„çº§:")
        print("-" * 40)
        
        # åŸºäºŽå“åº”æ—¶é—´çš„è¯„çº§
        expectation_times = [
            self.results.get('expectation_comparison_all', {}).get('duration', 0),
            self.results.get('expectation_comparison_1y', {}).get('duration', 0),
            self.results.get('expectation_comparison_90d', {}).get('duration', 0),
            self.results.get('expectation_comparison_30d', {}).get('duration', 0)
        ]
        
        avg_expectation_time = statistics.mean([t for t in expectation_times if t > 0])
        
        if avg_expectation_time < 0.1:
            time_rating = "ä¼˜ç§€"
        elif avg_expectation_time < 0.5:
            time_rating = "è‰¯å¥½"
        elif avg_expectation_time < 1.0:
            time_rating = "ä¸€èˆ¬"
        else:
            time_rating = "éœ€è¦ä¼˜åŒ–"
        
        print(f"  å“åº”æ—¶é—´è¯„çº§: {time_rating} (å¹³å‡ {avg_expectation_time:.3f}ç§’)")
        
        # åŸºäºŽå†…å­˜ä½¿ç”¨çš„è¯„çº§
        if 'memory_usage_summary' in self.results:
            memory_increase = self.results['memory_usage_summary']['total_increase_mb']
            
            if memory_increase < 10:
                memory_rating = "ä¼˜ç§€"
            elif memory_increase < 50:
                memory_rating = "è‰¯å¥½"
            elif memory_increase < 100:
                memory_rating = "ä¸€èˆ¬"
            else:
                memory_rating = "éœ€è¦ä¼˜åŒ–"
            
            print(f"  å†…å­˜ä½¿ç”¨è¯„çº§: {memory_rating} (å¢žé•¿ {memory_increase:.2f}MB)")
        
        # å¹¶å‘æ€§èƒ½è¯„çº§
        concurrent_results = [
            self.results.get('concurrent_access_5_threads', {}).get('duration', 0),
            self.results.get('concurrent_access_10_threads', {}).get('duration', 0),
            self.results.get('concurrent_access_20_threads', {}).get('duration', 0)
        ]
        
        max_concurrent_time = max([t for t in concurrent_results if t > 0])
        
        if max_concurrent_time < 2.0:
            concurrent_rating = "ä¼˜ç§€"
        elif max_concurrent_time < 5.0:
            concurrent_rating = "è‰¯å¥½"
        elif max_concurrent_time < 10.0:
            concurrent_rating = "ä¸€èˆ¬"
        else:
            concurrent_rating = "éœ€è¦ä¼˜åŒ–"
        
        print(f"  å¹¶å‘æ€§èƒ½è¯„çº§: {concurrent_rating} (æœ€å¤§ {max_concurrent_time:.3f}ç§’)")
    
    def save_detailed_report(self):
        """ä¿å­˜è¯¦ç»†æŠ¥å‘Šåˆ°æ–‡ä»¶"""
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f'performance_benchmark_report_{timestamp}.json'
        
        report = {
            'timestamp': datetime.now().isoformat(),
            'system_info': {
                'cpu_count': psutil.cpu_count(),
                'memory_total': psutil.virtual_memory().total,
                'python_version': os.sys.version,
                'platform': os.name
            },
            'test_results': self.results,
            'summary': {
                'total_tests': len(self.results),
                'avg_response_time': statistics.mean([r.get('duration', 0) for r in self.results.values()]),
                'total_memory_usage': sum([r.get('memory_delta', 0) for r in self.results.values()])
            }
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print(f"\nè¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {filename}")


def run_performance_benchmark():
    """è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•çš„ä¸»å‡½æ•°"""
    
    try:
        benchmark = PerformanceBenchmark()
        benchmark.run_benchmark_suite()
        
        print("\nðŸŽ‰ æ€§èƒ½åŸºå‡†æµ‹è¯•å®Œæˆï¼")
        return True
        
    except Exception as e:
        print(f"\nâŒ æ€§èƒ½åŸºå‡†æµ‹è¯•å¤±è´¥: {str(e)}")
        return False


if __name__ == '__main__':
    success = run_performance_benchmark()
    exit(0 if success else 1)