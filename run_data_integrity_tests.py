#!/usr/bin/env python3
"""
æ•°æ®å®Œæ•´æ€§å’Œå…¼å®¹æ€§æµ‹è¯•è¿è¡Œå™¨
æ‰§è¡Œä»»åŠ¡11çš„æ‰€æœ‰æµ‹è¯•è¦æ±‚

Requirements: 8.1, 8.2, 8.3, 8.4, 8.5
- éªŒè¯ç°æœ‰analyticsåŠŸèƒ½ä¸å—å½±å“
- æµ‹è¯•æ–°åŠŸèƒ½å¯¹ç°æœ‰æ•°æ®çš„åªè¯»è®¿é—®
- éªŒè¯é”™è¯¯æƒ…å†µä¸‹çš„ç³»ç»Ÿç¨³å®šæ€§
- æµ‹è¯•ä¸åŒæ•°æ®é‡ä¸‹çš„æ€§èƒ½è¡¨ç°
"""

import sys
import os
import subprocess
import json
import time
from datetime import datetime
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))


class DataIntegrityTestRunner:
    """æ•°æ®å®Œæ•´æ€§æµ‹è¯•è¿è¡Œå™¨"""
    
    def __init__(self):
        self.test_results = {}
        self.start_time = None
        self.end_time = None
        
    def run_all_tests(self):
        """è¿è¡Œæ‰€æœ‰æ•°æ®å®Œæ•´æ€§å’Œå…¼å®¹æ€§æµ‹è¯•"""
        
        print("=" * 80)
        print("æ•°æ®å®Œæ•´æ€§å’Œå…¼å®¹æ€§æµ‹è¯•å¥—ä»¶")
        print("=" * 80)
        print(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print()
        
        self.start_time = time.time()
        
        # æµ‹è¯•åˆ—è¡¨
        tests = [
            {
                'name': 'åç«¯æ•°æ®å®Œæ•´æ€§æµ‹è¯•',
                'description': 'éªŒè¯ç°æœ‰analyticsåŠŸèƒ½ä¸å—å½±å“ï¼Œæµ‹è¯•åªè¯»è®¿é—®',
                'method': self.run_backend_integrity_tests,
                'requirements': ['8.1', '8.2', '8.3']
            },
            {
                'name': 'å‰ç«¯å…¼å®¹æ€§æµ‹è¯•',
                'description': 'éªŒè¯å‰ç«¯åŠŸèƒ½å…¼å®¹æ€§å’Œç”¨æˆ·ç•Œé¢ç¨³å®šæ€§',
                'method': self.run_frontend_compatibility_tests,
                'requirements': ['8.5']
            },
            {
                'name': 'é”™è¯¯å¤„ç†ç¨³å®šæ€§æµ‹è¯•',
                'description': 'éªŒè¯é”™è¯¯æƒ…å†µä¸‹çš„ç³»ç»Ÿç¨³å®šæ€§',
                'method': self.run_error_handling_tests,
                'requirements': ['8.4']
            },
            {
                'name': 'æ€§èƒ½åŸºå‡†æµ‹è¯•',
                'description': 'æµ‹è¯•ä¸åŒæ•°æ®é‡ä¸‹çš„æ€§èƒ½è¡¨ç°',
                'method': self.run_performance_tests,
                'requirements': ['8.5']
            },
            {
                'name': 'APIå…¼å®¹æ€§æµ‹è¯•',
                'description': 'éªŒè¯APIæ¥å£çš„å‘åå…¼å®¹æ€§',
                'method': self.run_api_compatibility_tests,
                'requirements': ['8.1', '8.2']
            },
            {
                'name': 'æ•°æ®åº“äº‹åŠ¡æµ‹è¯•',
                'description': 'éªŒè¯æ•°æ®åº“æ“ä½œçš„äº‹åŠ¡å®‰å…¨æ€§',
                'method': self.run_database_transaction_tests,
                'requirements': ['8.3']
            }
        ]
        
        # æ‰§è¡Œæ‰€æœ‰æµ‹è¯•
        for i, test in enumerate(tests, 1):
            print(f"[{i}/{len(tests)}] {test['name']}")
            print(f"æè¿°: {test['description']}")
            print(f"éœ€æ±‚: {', '.join(test['requirements'])}")
            print("-" * 60)
            
            try:
                result = test['method']()
                self.test_results[test['name']] = result
                
                if result['success']:
                    print(f"âœ“ {test['name']} é€šè¿‡")
                else:
                    print(f"âœ— {test['name']} å¤±è´¥: {result.get('message', 'æœªçŸ¥é”™è¯¯')}")
                    
            except Exception as e:
                print(f"âœ— {test['name']} å¼‚å¸¸: {str(e)}")
                self.test_results[test['name']] = {
                    'success': False,
                    'message': f'æµ‹è¯•æ‰§è¡Œå¼‚å¸¸: {str(e)}',
                    'exception': True
                }
            
            print()
        
        self.end_time = time.time()
        
        # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
        self.generate_test_report()
        
        # è¿”å›æ€»ä½“æµ‹è¯•ç»“æœ
        return self.calculate_overall_result()
    
    def run_backend_integrity_tests(self):
        """è¿è¡Œåç«¯æ•°æ®å®Œæ•´æ€§æµ‹è¯•"""
        
        try:
            # è¿è¡Œpytestæµ‹è¯•
            result = subprocess.run([
                sys.executable, '-m', 'pytest', 
                'tests/test_data_integrity_compatibility.py',
                '-v', '--tb=short'
            ], capture_output=True, text=True, cwd=project_root)
            
            if result.returncode == 0:
                return {
                    'success': True,
                    'message': 'æ‰€æœ‰åç«¯å®Œæ•´æ€§æµ‹è¯•é€šè¿‡',
                    'details': result.stdout
                }
            else:
                return {
                    'success': False,
                    'message': 'éƒ¨åˆ†åç«¯å®Œæ•´æ€§æµ‹è¯•å¤±è´¥',
                    'details': result.stderr,
                    'stdout': result.stdout
                }
                
        except FileNotFoundError:
            # å¦‚æœpytestä¸å¯ç”¨ï¼Œå°è¯•ç›´æ¥è¿è¡Œæµ‹è¯•æ–‡ä»¶
            try:
                result = subprocess.run([
                    sys.executable, 'tests/test_data_integrity_compatibility.py'
                ], capture_output=True, text=True, cwd=project_root)
                
                return {
                    'success': result.returncode == 0,
                    'message': 'åç«¯å®Œæ•´æ€§æµ‹è¯•å®Œæˆ' if result.returncode == 0 else 'åç«¯å®Œæ•´æ€§æµ‹è¯•å¤±è´¥',
                    'details': result.stdout if result.returncode == 0 else result.stderr
                }
                
            except Exception as e:
                return {
                    'success': False,
                    'message': f'æ— æ³•è¿è¡Œåç«¯å®Œæ•´æ€§æµ‹è¯•: {str(e)}'
                }
    
    def run_frontend_compatibility_tests(self):
        """è¿è¡Œå‰ç«¯å…¼å®¹æ€§æµ‹è¯•"""
        
        # æ£€æŸ¥å‰ç«¯æµ‹è¯•æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        frontend_test_file = project_root / 'test_frontend_compatibility.html'
        
        if not frontend_test_file.exists():
            return {
                'success': False,
                'message': 'å‰ç«¯å…¼å®¹æ€§æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨'
            }
        
        # æ£€æŸ¥å…³é”®çš„å‰ç«¯æ–‡ä»¶
        required_files = [
            'templates/analytics.html',
            'static/js/expectation-comparison-manager.js'
        ]
        
        missing_files = []
        for file_path in required_files:
            if not (project_root / file_path).exists():
                missing_files.append(file_path)
        
        if missing_files:
            return {
                'success': False,
                'message': f'ç¼ºå°‘å…³é”®å‰ç«¯æ–‡ä»¶: {", ".join(missing_files)}'
            }
        
        # éªŒè¯HTMLæ–‡ä»¶ç»“æ„
        try:
            with open(frontend_test_file, 'r', encoding='utf-8') as f:
                content = f.read()
                
            # æ£€æŸ¥å…³é”®å…ƒç´ 
            required_elements = [
                'test-analytics-tabs',
                'test-overview-tab',
                'test-expectation-tab',
                'FrontendCompatibilityTester'
            ]
            
            missing_elements = []
            for element in required_elements:
                if element not in content:
                    missing_elements.append(element)
            
            if missing_elements:
                return {
                    'success': False,
                    'message': f'å‰ç«¯æµ‹è¯•æ–‡ä»¶ç¼ºå°‘å…³é”®å…ƒç´ : {", ".join(missing_elements)}'
                }
            
            return {
                'success': True,
                'message': 'å‰ç«¯å…¼å®¹æ€§æµ‹è¯•æ–‡ä»¶éªŒè¯é€šè¿‡',
                'details': f'æµ‹è¯•æ–‡ä»¶ä½ç½®: {frontend_test_file}'
            }
            
        except Exception as e:
            return {
                'success': False,
                'message': f'å‰ç«¯å…¼å®¹æ€§æµ‹è¯•éªŒè¯å¤±è´¥: {str(e)}'
            }
    
    def run_error_handling_tests(self):
        """è¿è¡Œé”™è¯¯å¤„ç†ç¨³å®šæ€§æµ‹è¯•"""
        
        try:
            # åˆ›å»ºä¸´æ—¶æµ‹è¯•è„šæœ¬
            test_script = """
import sys
import os
sys.path.insert(0, '.')

from flask import Flask
from unittest.mock import patch
from services.expectation_comparison_service import ExpectationComparisonService
from api.analytics_routes import api_bp

def test_error_handling():
    app = Flask(__name__)
    app.register_blueprint(api_bp, url_prefix='/api')
    client = app.test_client()
    
    errors_handled = 0
    total_tests = 0
    
    with app.app_context():
        # æµ‹è¯•æ— æ•ˆå‚æ•°
        total_tests += 1
        response = client.get('/api/analytics/expectation-comparison?time_range=invalid')
        if response.status_code in [400, 500]:
            errors_handled += 1
        
        # æµ‹è¯•æ•°æ®åº“å¼‚å¸¸
        total_tests += 1
        with patch('models.trade_record.TradeRecord.query') as mock_query:
            mock_query.side_effect = Exception("Database error")
            response = client.get('/api/analytics/expectation-comparison')
            if response.status_code == 500:
                errors_handled += 1
        
        # æµ‹è¯•æœåŠ¡å±‚å¼‚å¸¸
        total_tests += 1
        with patch.object(ExpectationComparisonService, 'get_expectation_comparison') as mock_service:
            mock_service.side_effect = Exception("Service error")
            response = client.get('/api/analytics/expectation-comparison')
            if response.status_code == 500:
                errors_handled += 1
    
    return errors_handled, total_tests

if __name__ == '__main__':
    handled, total = test_error_handling()
    print(f"é”™è¯¯å¤„ç†æµ‹è¯•: {handled}/{total} é€šè¿‡")
    exit(0 if handled == total else 1)
"""
            
            # å†™å…¥ä¸´æ—¶æ–‡ä»¶
            temp_file = project_root / 'temp_error_test.py'
            with open(temp_file, 'w', encoding='utf-8') as f:
                f.write(test_script)
            
            try:
                # è¿è¡Œæµ‹è¯•
                result = subprocess.run([
                    sys.executable, str(temp_file)
                ], capture_output=True, text=True, cwd=project_root)
                
                return {
                    'success': result.returncode == 0,
                    'message': 'é”™è¯¯å¤„ç†æµ‹è¯•å®Œæˆ' if result.returncode == 0 else 'é”™è¯¯å¤„ç†æµ‹è¯•å¤±è´¥',
                    'details': result.stdout
                }
                
            finally:
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                if temp_file.exists():
                    temp_file.unlink()
                    
        except Exception as e:
            return {
                'success': False,
                'message': f'é”™è¯¯å¤„ç†æµ‹è¯•å¼‚å¸¸: {str(e)}'
            }
    
    def run_performance_tests(self):
        """è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•"""
        
        try:
            # è¿è¡Œæ€§èƒ½æµ‹è¯•
            result = subprocess.run([
                sys.executable, 'test_performance_benchmark.py'
            ], capture_output=True, text=True, cwd=project_root)
            
            return {
                'success': result.returncode == 0,
                'message': 'æ€§èƒ½åŸºå‡†æµ‹è¯•å®Œæˆ' if result.returncode == 0 else 'æ€§èƒ½åŸºå‡†æµ‹è¯•å¤±è´¥',
                'details': result.stdout if result.returncode == 0 else result.stderr
            }
            
        except Exception as e:
            return {
                'success': False,
                'message': f'æ€§èƒ½æµ‹è¯•å¼‚å¸¸: {str(e)}'
            }
    
    def run_api_compatibility_tests(self):
        """è¿è¡ŒAPIå…¼å®¹æ€§æµ‹è¯•"""
        
        try:
            # æ£€æŸ¥APIè·¯ç”±æ–‡ä»¶
            api_file = project_root / 'api' / 'analytics_routes.py'
            
            if not api_file.exists():
                return {
                    'success': False,
                    'message': 'APIè·¯ç”±æ–‡ä»¶ä¸å­˜åœ¨'
                }
            
            # éªŒè¯APIç«¯ç‚¹
            with open(api_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            required_endpoints = [
                '/analytics/overview',
                '/analytics/profit-distribution',
                '/analytics/monthly',
                '/analytics/expectation-comparison'
            ]
            
            missing_endpoints = []
            for endpoint in required_endpoints:
                if endpoint not in content:
                    missing_endpoints.append(endpoint)
            
            if missing_endpoints:
                return {
                    'success': False,
                    'message': f'ç¼ºå°‘APIç«¯ç‚¹: {", ".join(missing_endpoints)}'
                }
            
            # æ£€æŸ¥å“åº”æ ¼å¼ä¸€è‡´æ€§
            if 'create_success_response' not in content or 'create_error_response' not in content:
                return {
                    'success': False,
                    'message': 'APIå“åº”æ ¼å¼ä¸ä¸€è‡´'
                }
            
            return {
                'success': True,
                'message': 'APIå…¼å®¹æ€§éªŒè¯é€šè¿‡',
                'details': f'éªŒè¯äº† {len(required_endpoints)} ä¸ªAPIç«¯ç‚¹'
            }
            
        except Exception as e:
            return {
                'success': False,
                'message': f'APIå…¼å®¹æ€§æµ‹è¯•å¼‚å¸¸: {str(e)}'
            }
    
    def run_database_transaction_tests(self):
        """è¿è¡Œæ•°æ®åº“äº‹åŠ¡æµ‹è¯•"""
        
        try:
            # åˆ›å»ºæ•°æ®åº“äº‹åŠ¡æµ‹è¯•è„šæœ¬
            test_script = """
import sys
sys.path.insert(0, '.')

from models.trade_record import TradeRecord
from services.expectation_comparison_service import ExpectationComparisonService

def test_readonly_access():
    # è®°å½•åˆå§‹çŠ¶æ€
    initial_count = TradeRecord.query.count()
    
    # æ‰§è¡Œåªè¯»æ“ä½œ
    try:
        ExpectationComparisonService.get_expectation_comparison('all')
        ExpectationComparisonService.get_expectation_comparison('1y')
        ExpectationComparisonService.get_expectation_comparison('90d')
        ExpectationComparisonService.get_expectation_comparison('30d')
    except Exception as e:
        print(f"åªè¯»æ“ä½œå¼‚å¸¸: {e}")
        return False
    
    # éªŒè¯æ•°æ®æœªè¢«ä¿®æ”¹
    final_count = TradeRecord.query.count()
    
    if initial_count != final_count:
        print(f"æ•°æ®è¢«æ„å¤–ä¿®æ”¹: {initial_count} -> {final_count}")
        return False
    
    print("æ•°æ®åº“äº‹åŠ¡æµ‹è¯•é€šè¿‡")
    return True

if __name__ == '__main__':
    success = test_readonly_access()
    exit(0 if success else 1)
"""
            
            # å†™å…¥ä¸´æ—¶æ–‡ä»¶
            temp_file = project_root / 'temp_db_test.py'
            with open(temp_file, 'w', encoding='utf-8') as f:
                f.write(test_script)
            
            try:
                # è¿è¡Œæµ‹è¯•
                result = subprocess.run([
                    sys.executable, str(temp_file)
                ], capture_output=True, text=True, cwd=project_root)
                
                return {
                    'success': result.returncode == 0,
                    'message': 'æ•°æ®åº“äº‹åŠ¡æµ‹è¯•å®Œæˆ' if result.returncode == 0 else 'æ•°æ®åº“äº‹åŠ¡æµ‹è¯•å¤±è´¥',
                    'details': result.stdout
                }
                
            finally:
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                if temp_file.exists():
                    temp_file.unlink()
                    
        except Exception as e:
            return {
                'success': False,
                'message': f'æ•°æ®åº“äº‹åŠ¡æµ‹è¯•å¼‚å¸¸: {str(e)}'
            }
    
    def calculate_overall_result(self):
        """è®¡ç®—æ€»ä½“æµ‹è¯•ç»“æœ"""
        
        total_tests = len(self.test_results)
        passed_tests = sum(1 for result in self.test_results.values() if result.get('success', False))
        
        return {
            'success': passed_tests == total_tests,
            'total_tests': total_tests,
            'passed_tests': passed_tests,
            'failed_tests': total_tests - passed_tests,
            'success_rate': (passed_tests / total_tests * 100) if total_tests > 0 else 0
        }
    
    def generate_test_report(self):
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        
        print("=" * 80)
        print("æµ‹è¯•ç»“æœæ‘˜è¦")
        print("=" * 80)
        
        overall = self.calculate_overall_result()
        
        print(f"æ€»æµ‹è¯•æ•°: {overall['total_tests']}")
        print(f"é€šè¿‡æµ‹è¯•: {overall['passed_tests']}")
        print(f"å¤±è´¥æµ‹è¯•: {overall['failed_tests']}")
        print(f"æˆåŠŸç‡: {overall['success_rate']:.1f}%")
        print(f"æ€»è€—æ—¶: {self.end_time - self.start_time:.2f}ç§’")
        print()
        
        # è¯¦ç»†ç»“æœ
        print("è¯¦ç»†æµ‹è¯•ç»“æœ:")
        print("-" * 60)
        
        for test_name, result in self.test_results.items():
            status = "âœ“ é€šè¿‡" if result.get('success', False) else "âœ— å¤±è´¥"
            print(f"{status} {test_name}")
            
            if not result.get('success', False):
                print(f"    é”™è¯¯: {result.get('message', 'æœªçŸ¥é”™è¯¯')}")
            
            if result.get('details'):
                # åªæ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯çš„å‰å‡ è¡Œ
                details_lines = result['details'].split('\n')[:3]
                for line in details_lines:
                    if line.strip():
                        print(f"    {line.strip()}")
        
        print()
        
        # éœ€æ±‚è¦†ç›–æƒ…å†µ
        print("éœ€æ±‚è¦†ç›–æƒ…å†µ:")
        print("-" * 60)
        
        requirements_coverage = {
            '8.1': 'éªŒè¯ç°æœ‰analyticsåŠŸèƒ½ä¸å—å½±å“',
            '8.2': 'æµ‹è¯•æ–°åŠŸèƒ½å¯¹ç°æœ‰æ•°æ®çš„åªè¯»è®¿é—®',
            '8.3': 'éªŒè¯é”™è¯¯æƒ…å†µä¸‹çš„ç³»ç»Ÿç¨³å®šæ€§',
            '8.4': 'æµ‹è¯•ä¸åŒæ•°æ®é‡ä¸‹çš„æ€§èƒ½è¡¨ç°',
            '8.5': 'ç¡®ä¿ç³»ç»Ÿæ•´ä½“å…¼å®¹æ€§'
        }
        
        for req_id, description in requirements_coverage.items():
            print(f"éœ€æ±‚ {req_id}: {description} - âœ“ å·²æµ‹è¯•")
        
        # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
        self.save_report_to_file(overall)
        
        print()
        if overall['success']:
            print("ğŸ‰ æ‰€æœ‰æ•°æ®å®Œæ•´æ€§å’Œå…¼å®¹æ€§æµ‹è¯•é€šè¿‡ï¼")
        else:
            print("âš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç³»ç»Ÿå…¼å®¹æ€§")
    
    def save_report_to_file(self, overall):
        """ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶"""
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        report_file = project_root / f'data_integrity_test_report_{timestamp}.json'
        
        report = {
            'timestamp': datetime.now().isoformat(),
            'overall_result': overall,
            'test_results': self.test_results,
            'execution_time': self.end_time - self.start_time,
            'requirements_tested': ['8.1', '8.2', '8.3', '8.4', '8.5']
        }
        
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print(f"è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")


def main():
    """ä¸»å‡½æ•°"""
    
    try:
        runner = DataIntegrityTestRunner()
        overall_result = runner.run_all_tests()
        
        # æ ¹æ®æµ‹è¯•ç»“æœè®¾ç½®é€€å‡ºç 
        exit_code = 0 if overall_result['success'] else 1
        sys.exit(exit_code)
        
    except KeyboardInterrupt:
        print("\næµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(1)
        
    except Exception as e:
        print(f"\næµ‹è¯•è¿è¡Œå™¨å¼‚å¸¸: {str(e)}")
        sys.exit(1)


if __name__ == '__main__':
    main()